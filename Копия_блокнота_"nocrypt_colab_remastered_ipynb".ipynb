{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "iLsO_riJrAEl",
        "QAvV1XNM3TxR"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slonpro/camTg-notification/blob/master/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22nocrypt_colab_remastered_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NoCrypt's Colab Remastered üîÆ**\n",
        "![](https://visitor-badge.glitch.me/badge?page_id=nocrypt.remastered) [![](https://dcbadge.vercel.app/api/shield/442099748669751297?style=flat)](https://lookup.guru/442099748669751297) [![ko-fi](https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/nocrypt)\n",
        "\n",
        "\n",
        "Yet another advanced n opinionated colab for [stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui), except it's ***quick af***‚ö°\n",
        "\n",
        "[Guide for using krita with this colab](https://rentry.org/krita-ext-colab)\n",
        "\n",
        "<!-- [Hey you, thanks for looking into my colab, please ignore message below, I just added it to announce stuff faster] -->\n",
        "<!-- # <font color=red> Testing needed: Merge Models Fix üëà -->\n",
        "<!-- # <font color=red>BROKEN FOR NOW, WILL FIX ASAP ‚ö†Ô∏è -->\n",
        "<!-- # <font color=red>Under construction ‚ö†Ô∏è -->\n",
        "\n",
        "<small><font color=gray>v23-03-17A | [For ppl who copied this colab, check updates here](https://colab.research.google.com/drive/1wEa-tS10h4LlDykd87TF5zzpXIIQoCmq)</small></font>\n",
        "\n",
        "<details>\n",
        "  <summary><big>Latest Changes</big></big></summary>\n",
        "\n",
        "  + Changes to output name to be more meaningful\n",
        "  + Attempt to fix broken start link\n",
        "  - Disabled openpose-editor\n",
        "  - Disabled sd-webui-depth-lib\n",
        "  - Deleted Easter Egg (will add more in future without noticing)\n",
        "\n",
        "---\n",
        "\n",
        "  + Change defaults: negative prompts, max res, max batch size\n",
        "  + Added sd-webui-runtime-block-merge\n",
        "  + Added multidiffusion-upscaler-for-automatic1111\n",
        "  + Added stable-diffusion-webui-image-filters\n",
        "  + Added sd-dynamic-prompts + wildcards (credit: CtD)\n",
        "  + Fixed configs_in_drive and output_to_drive can't work together\n",
        "  + Fixed merge conflict issue that preventing webui from starting\n",
        "  + Updated xformers\n",
        "  + Added sd-webui-tunnels (fork by camenduru)\n",
        "  + Changed `stable-diffusion-webui-images-browser` extension to `sd_images_browser` (fork by aka7774) \n",
        "  + Added sd-fast-pnginfo extension\n",
        "  + Replaced blessed_vae -> blessed2_vae\n",
        "  + Adapted to python 3.9 update\n",
        "  + Added t2iadapter models (including style)\n",
        "  + Increase ControlNet Max Models to 4\n",
        "  + New Default Extension: [sd-webui-llul](https://github.com/hnmr293/sd-webui-llul)\n",
        "  + Fixed broken img2img  \n",
        "\n",
        "  + Added Composable Lora extension\n",
        "  + Added Custom tags + easter eggs :>\n",
        "  + Added poseX and stable-diffusion-webui-fix-image-paste extensions\n",
        "  + Added Colab Mobile Keep Alive cell (in useful utils, run it before start cell)\n",
        "  + Fixed extra networks tab after re-ordering tool buttons\n",
        "  + Added an option to disable notification sound in webui\n",
        "  + Lowered the notification sound by 40%. (thanks RileyX for the suggestion)\n",
        "  + Replaced two_shot_ext options (already built in)\n",
        "  + All_ControlNet is now enabled by default\n",
        "  + Notification sounds when finished generating, forked from https://github.com/etherealxx/batchlinks-webui\n",
        "  + New default extensions:\n",
        "    + stable-diffusion-webui-two-shot\n",
        "    + sd-webui-supermerger\n",
        "    + batchlinks-webui\n",
        "    + a1111-sd-webui-locon\n",
        "    + sd-webui-depth-lib\n",
        "    + openpose-editor\n",
        "\n",
        "  + Manual Downloads is now supported. Guide soon!\n",
        "  + Added Waifu Diffusion Beta2 Aesthetic\n",
        "  + Replaced WD 1.5 Beta -> Waifu Diffusion Beta2\n",
        "  + Fixed SD vae\n",
        "  + New option: fast_start. This will make the startup speed faster by bypassing dependency installations (will break extensions, use with caution)\n",
        "  + New option: configs_in_drive. This will use configs (config, uiconfig, styles) in your drive\n",
        "  + Changed git pull logic to first install only for saving time (will add old option soon)\n",
        "  + Updated xformers, deps, and some UI settings\n",
        "  + New model: All DIFF control models (see the differences [here](https://huggingface.co/kohya-ss/ControlNet-diff-modules/discussions/1))\n",
        "  + Fixed umiAI and two shot can't be installed together (Thanks Aojiru)\n",
        "  + Fixed control models hardlink (Thanks Aojiru)\n",
        "  + New model: All Control Models (fp16)\n",
        "  + New option: Two_shot Extension\n",
        "  + Support for control models in `custom_urls`\n",
        "  + Added controlnet extension by default\n",
        "  + Preload with self-trimmed and self-converted safetensors openpose model (+openpose hands and body pth models)\n",
        "  - Installation speed slowed by few seconds due to models addition (1.7gb -> 3gb)\n",
        "  - Changed console text to cyan\n",
        "  - Model replaced: WD 1.4 e1 -> WD 1.5 Beta\n",
        "  - New option: UmiAI. Now it's a separate option, no more bloated embeddings :D\n",
        "  - New option: Update extensions. Basically updating all the extension the latest version (Might be slow)\n",
        "  - New option: Colab Optimization. (basically Load in vram + aggressive gradio queue)\n",
        "  - Added support for old commits by removing inbuilt negative embeddings. (if you use commit_hash that is)\n",
        "  - Gradio server is turned off when `alternative tunnel` or `ngrok` is active.\n",
        "  \n",
        "</details>"
      ],
      "metadata": {
        "id": "mgKX41doZJ3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Uvqba0yKW7U"
      },
      "outputs": [],
      "source": [
        "#@title # **Start** üöÄ \n",
        "# All necessary imports goes here\n",
        "from IPython.display import clear_output, display, HTML\n",
        "import os\n",
        "import time\n",
        "from datetime import timedelta\n",
        "from google.colab import drive\n",
        "from IPython.utils import capture\n",
        "from subprocess import getoutput\n",
        "from urllib.parse import unquote\n",
        "%cd /content\n",
        "try:\n",
        "  start_colab\n",
        "except:\n",
        "  start_colab = int(time.time())-5\n",
        "clear_output()\n",
        "print(\"\\033[96m\") #Cyan text\n",
        "\n",
        "# Check if gpu exist, stop if don't.\n",
        "try:\n",
        "  output \n",
        "except:\n",
        "  print('‚åö Checking GPU...', end='')\n",
        "  output = getoutput('nvidia-smi --query-gpu=gpu_name --format=csv')\n",
        "  if \"name\" in output:\n",
        "    gpu_name = output[5:]\n",
        "    print('\\r‚úÖ Current GPU:', gpu_name, flush=True)\n",
        "  else:\n",
        "    print('\\r\\033[91m‚ùé ERROR: No GPU detected. Please do step below to enable.\\n', flush=True)\n",
        "    display(HTML(\"<img src='https://i.ibb.co/HC9KH17/NVIDIA-Share-23-01-02-173037.png' width='800px'/>\"))\n",
        "    print('\\033[91m\\nIf it says \"Cannot connect to GPU backend\", meaning you\\'ve either reached free usage limit. OR there\\'s no gpu available.\\n\\nDon\\'t mind me... I\\'m destroying your current session for your own good...')\n",
        "    display(HTML(\"<img src='https://media.tenor.com/E9omRGF7x0AAAAAC/hitori-gotou-bocchi-rock.gif' width='500px'/>\"))\n",
        "    time.sleep(5)\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()\n",
        "\n",
        "# [ALL PARAMS]-----------------------------------------------\n",
        "#@markdown ### **Install Configurations &nbsp;&nbsp;&nbsp;[?](https://rentry.org/ncpt_what)** \n",
        "latest_webui = True #@param{type:\"boolean\"}\n",
        "latest_extensions = True #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown ### <br> **Configurations**\n",
        "output_to_drive = False #@param{type:\"boolean\"}\n",
        "configs_in_drive = False #@param{type:\"boolean\"}\n",
        "fast_start = False #@param{type:\"boolean\"}\n",
        "auto_vae = False #@param{type:\"boolean\"}\n",
        "no_custom_theme = False #@param {type:\"boolean\"} \n",
        "merge_in_vram = True #@param {type:\"boolean\"} \n",
        "colab_optimizations = True #@param {type:\"boolean\"} \n",
        "ram_patch_for_sd2 = True #@param{type:\"boolean\"}\n",
        "krita_paint_ext = False #@param {type:\"boolean\"} \n",
        "umiai_ext = False #@param {type:\"boolean\"} \n",
        "# two_shot_ext = False #@param {type:\"boolean\"} \n",
        "verbose_download = False #@param {type:\"boolean\"} \n",
        "commandline_arguments = \"--enable-insecure-extension-access --share --xformers --no-half-vae --disable-safe-unpickle --theme dark --no-hashing --lowram --gradio-queue\" #@param{type:\"string\"}\n",
        "commit_hash = \"\" #@param{type:\"string\"}\n",
        "ngrok_token  = \"\" #@param{type:\"string\"}\n",
        "ngrok_region = \"jp\" #@param [\"us\", \"eu\", \"au\", \"ap\", \"sa\", \"jp\", \"in\"]\n",
        "ngrok_auto_save_load = False #@param{type:\"boolean\"}\n",
        "alternative_tunnels = False #@param{type:\"boolean\"}\n",
        "with_bore = False #@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown ### <br> **Models, VAEs, Embeddings, Hypernetworks, Yaml, LoRA**\n",
        "optional_huggingface_token=\"\" #@param{type:\"string\"}\n",
        "model_url = \"\"\n",
        "waifu_diffusion_beta2_AES = False #@param{type:\"boolean\"}\n",
        "waifu_diffusion_beta2 = False #@param{type:\"boolean\"}\n",
        "anything_v3 = False #@param{type:\"boolean\"}\n",
        "anything_v4_5 = True #@param{type:\"boolean\"}\n",
        "something_v2_2 = False #@param{type:\"boolean\"}\n",
        "anything_vae = False #@param{type:\"boolean\"}\n",
        "blessed2_vae = False #@param{type:\"boolean\"}\n",
        "wd_vae = False #@param{type:\"boolean\"}\n",
        "sd_vae = False #@param{type:\"boolean\"}\n",
        "all_control_models = True #@param{type:\"boolean\"}\n",
        "all_diff_control_models = False #@param{type:\"boolean\"}\n",
        "all_t2iadapter_models = False #@param{type:\"boolean\"}\n",
        "null_model = False #@param{type:\"boolean\"}\n",
        "custom_urls = \"\" #@param {type:\"string\"}\n",
        "#@markdown &nbsp; &nbsp; <font color=gray><i> Put any models, embeddings, configs, hypernetworks, loras links and separate it with comma <br>\n",
        "#@markdown &nbsp; &nbsp;<img src=\"https://cdn.discordapp.com/emojis/930580027135901777.webp?size=56\" width=\"18\"/> [How to use this custom url box?](https://rentry.org/custom_url_nocrypt) - [**Awesome Models List**](https://rentry.org/ncpt_fav_models) ‚ú®\n",
        "\n",
        "# CONFIG DIR (not recommended to change unless you know what you're doing)\n",
        "destination_dir = \"/content/.downloaded/\"\n",
        "config_dir=\"/content/stable-diffusion-webui/config.json\"\n",
        "models_dir = \"/content/stable-diffusion-webui/models/Stable-diffusion/\"\n",
        "vaes_dir = \"/content/stable-diffusion-webui/models/VAE/\"\n",
        "hypernetworks_dir = \"/content/stable-diffusion-webui/models/hypernetworks/\"\n",
        "embeddings_dir = \"/content/stable-diffusion-webui/embeddings/\"\n",
        "loras_dir = \"/content/stable-diffusion-webui/models/Lora\"\n",
        "patches_dir = \"/content/patches\"\n",
        "extensions_dir = \"/content/stable-diffusion-webui/extensions/\"\n",
        "control_dir = \"/content/stable-diffusion-webui/extensions/sd-webui-controlnet/models\"\n",
        "drive_config_dir = \"/content/gdrive/MyDrive/WebUI/configs/\"\n",
        "# -----------------------------------------------\n",
        "\n",
        "# Append models to model_url\n",
        "model_url+=custom_urls+\", \" if custom_urls else \"\"\n",
        "if anything_v3:\n",
        "  model_url+=\"https://huggingface.co/NoCrypt/safetensor_models/resolve/main/Anything-V3.0-pruned-fp32.safetensors, \"\n",
        "if anything_v4_5:\n",
        "  model_url+=\"https://huggingface.co/andite/anything-v4.0/resolve/main/anything-v4.5-pruned.safetensors, \"\n",
        "if something_v2_2:\n",
        "  model_url+=\"https://huggingface.co/NoCrypt/SomethingV2_2/resolve/main/SomethingV2_2.safetensors, \"\n",
        "if waifu_diffusion_beta2_AES:\n",
        "  model_url+=\"https://huggingface.co/waifu-diffusion/wd-1-5-beta2/resolve/main/checkpoints/wd-1-5-beta2-aesthetic-fp32.safetensors, https://huggingface.co/waifu-diffusion/wd-1-5-beta2/resolve/main/checkpoints/wd-1-5-beta2-aesthetic-fp32.yaml, \"\n",
        "if waifu_diffusion_beta2:\n",
        "  model_url+=\"https://huggingface.co/waifu-diffusion/wd-1-5-beta2/resolve/main/checkpoints/wd-1-5-beta2-fp32.safetensors, https://huggingface.co/waifu-diffusion/wd-1-5-beta2/resolve/main/checkpoints/wd-1-5-beta2-fp32.yaml, \"\n",
        "if anything_vae or anything_v3 or anything_v4_5:\n",
        "  model_url+=\" https://huggingface.co/NoCrypt/Anything-v3-0/resolve/main/anything.vae.pt, \"\n",
        "if blessed2_vae:\n",
        "  model_url+=\"https://huggingface.co/NoCrypt/blessed_vae/resolve/main/blessed2.vae.pt, \"\n",
        "if wd_vae or waifu_diffusion_beta2 or waifu_diffusion_beta2_AES:\n",
        "  model_url+=\"https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt, \"\n",
        "if sd_vae:\n",
        "  model_url+=\"https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors, \"\n",
        "if all_control_models:\n",
        "  model_url+=\"https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_canny-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_depth-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_hed-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_mlsd-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_normal-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_openpose-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_scribble-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/control_seg-fp16.safetensors, \"\n",
        "if all_diff_control_models:\n",
        "  model_url+=\"https://huggingface.co/kohya-ss/ControlNet-diff-modules/resolve/main/diff_control_sd15_canny_fp16.safetensors, https://huggingface.co/kohya-ss/ControlNet-diff-modules/resolve/main/diff_control_sd15_depth_fp16.safetensors, https://huggingface.co/kohya-ss/ControlNet-diff-modules/resolve/main/diff_control_sd15_hed_fp16.safetensors, https://huggingface.co/kohya-ss/ControlNet-diff-modules/resolve/main/diff_control_sd15_mlsd_fp16.safetensors, https://huggingface.co/kohya-ss/ControlNet-diff-modules/resolve/main/diff_control_sd15_normal_fp16.safetensors, https://huggingface.co/kohya-ss/ControlNet-diff-modules/resolve/main/diff_control_sd15_openpose_fp16.safetensors, https://huggingface.co/kohya-ss/ControlNet-diff-modules/resolve/main/diff_control_sd15_scribble_fp16.safetensors, https://huggingface.co/kohya-ss/ControlNet-diff-modules/resolve/main/diff_control_sd15_seg_fp16.safetensors, \"\n",
        "if all_t2iadapter_models:\n",
        "  model_url+=\"https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_canny-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_color-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_depth-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_keypose-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_openpose-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_seg-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_sketch-fp16.safetensors, https://huggingface.co/webui/ControlNet-modules-safetensors/resolve/main/t2iadapter_style-fp16.safetensors,\"\n",
        "\n",
        "# Image outputs to drive (part 1)\n",
        "if output_to_drive or configs_in_drive:\n",
        "  if not os.path.exists('/content/gdrive'):\n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "# Unpack Repo, Dependencies, Caches\n",
        "if not os.path.exists(\"/content/stable-diffusion-webui\"):\n",
        "  start_install = int(time.time())\n",
        "  print(\"üöÄ Unpacking... Please do not stop this process at all cost...\", end='')\n",
        "  with capture.capture_output() as cap:\n",
        "    !rm -rf /usr/local/lib/python3.9/dist-packages/scipy /usr/local/lib/python3.9/dist-packages/scipy-*.dist-info/ /usr/local/lib/python3.9/dist-packages/scipy.libs\n",
        "\n",
        "    !wget https://huggingface.co/NoCrypt/fast-repo/resolve/main/ubuntu_deps.zip && unzip ubuntu_deps.zip -d ./deps && dpkg -i ./deps/* && rm -rf ubuntu_deps.zip /content/deps/\n",
        "    # !apt install liblz4-tool aria2\n",
        "    # !pip uninstall -q -y huggingface_hub\n",
        "    # !{'curl -LO https://github.com/BurntSushi/ripgrep/releases/download/13.0.0/ripgrep_13.0.0_amd64.deb && dpkg -i ripgrep_13.0.0_amd64.deb && rm -rf ripgrep_13.0.0_amd64.deb'}\n",
        "    \n",
        "    # !aria2c --summary-interval=10 -c -x 16 -k 1M -s 16 -d /content -Z \\\n",
        "    #   https://huggingface.co/NoCrypt/fast-repo/resolve/main/dep.tar.lz4 \\\n",
        "    #   https://huggingface.co/NoCrypt/fast-repo/resolve/main/repo.tar.lz4 \\\n",
        "    #   https://huggingface.co/NoCrypt/fast-repo/resolve/main/cache.tar.lz4\n",
        "    \n",
        "    # !aria2c -d /content -o dep.tar.lz4 --summary-interval=10 -c -x 16 -k 1M -s 16 https://huggingface.co/NoCrypt/fast-repo/resolve/main/dep.tar.lz4\n",
        "    # !aria2c -d /content -o repo.tar.lz4 --summary-interval=10 -c -x 16 -k 1M -s 16 https://huggingface.co/NoCrypt/fast-repo/resolve/main/repo.tar.lz4\n",
        "    # !aria2c -d /content -o cache.tar.lz4 --summary-interval=10 -c -x 16 -k 1M -s 16 https://huggingface.co/NoCrypt/fast-repo/resolve/main/cache.tar.lz4\n",
        "    \n",
        "    !echo -e \"https://huggingface.co/NoCrypt/fast-repo/resolve/main/dep.tar.lz4\\n\\tout=dep.tar.lz4\\nhttps://huggingface.co/NoCrypt/fast-repo/resolve/main/repo.tar.lz4\\n\\tout=repo.tar.lz4\\nhttps://huggingface.co/NoCrypt/fast-repo/resolve/main/cache.tar.lz4\\n\\tout=cache.tar.lz4\\n\" \\\n",
        "      | aria2c -i- -j5 -x16 -s16 -k1M -c \n",
        "    \n",
        "    !tar -xI lz4 -f dep.tar.lz4 --overwrite-dir --directory=/usr/local/lib/python3.9/dist-packages/ #(manual dir)\n",
        "    !tar -xI lz4 -f repo.tar.lz4 --directory=/ #/content/stable-diffusion-webui/ (auto dir)\n",
        "    # !tar -xI lz4 -f cache.tar.lz4 --directory=/ #/root/.cache/huggingface (auto dir)\n",
        "    !rm -rf /content/dep.tar.lz4 /content/repo.tar.lz4 /content/cache.tar.lz4\n",
        "    os.environ[\"SAFETENSORS_FAST_GPU\"]='1'\n",
        "    del cap\n",
        "  if not os.path.exists(hypernetworks_dir):\n",
        "    os.makedirs(hypernetworks_dir)\n",
        "  # if not 'T4' in gpu_name:  # [For colab makers out there, facebook's xformers pypi already support almost all gpu now]\n",
        "  #   !pip uninstall -y xformers\n",
        "  install_time = timedelta(seconds=time.time()-start_install)\n",
        "  print(\"\\rüöÄ Finished unpacking. Took\",\"%02d:%02d:%02d ‚ö°\\n\" % (install_time.seconds / 3600, (install_time.seconds / 60) % 60, install_time.seconds % 60), end='', flush=True)\n",
        "  # Colab ü§ù Gradio (Colab timer integration for gradio)\n",
        "  !echo -n {start_colab} > /content/stable-diffusion-webui/static/colabTimer.txt\n",
        "  print(\"ü§ù Colab timer integration complete! You can see your colab time inside webui.\")\n",
        "\n",
        "  # Update using git pull\n",
        "  if latest_webui:\n",
        "    !git config --global user.email \"you@example.com\"\n",
        "    !git config --global user.name \"Your Name\"\n",
        "    print('‚åö Pulling latest changes...', end=\"\")\n",
        "    with capture.capture_output() as cap:\n",
        "      %cd /content/stable-diffusion-webui\n",
        "      !git pull -X theirs --rebase --autostash\n",
        "      del cap\n",
        "    print('\\rü™Ñ \\033[96mYou are currently using latest version of webui. Please use commit_hash if there is error', flush=True)\n",
        "\n",
        "  # Update extensions\n",
        "  if latest_extensions:\n",
        "    print('‚åö Updating extensions (might take a while)...', end=\"\")\n",
        "    with capture.capture_output() as cap:\n",
        "      !{'for dir in /content/stable-diffusion-webui/extensions/*/; do cd \"$dir\" && git fetch origin && git pull; done'}\n",
        "    del cap\n",
        "    print('\\rü™Ñ \\033[96mInbuilt extensions are updated to its latest versions', flush=True)\n",
        "else:\n",
        "  print(\"üöÄ Already unpacked... Skipping.\")\n",
        "  time_since_start = timedelta(seconds=time.time()-start_colab)\n",
        "  print(\"‚åö You've been running this colab for\",\"%02d:%02d:%02d\" % (time_since_start.seconds / 3600, (time_since_start.seconds / 60) % 60, time_since_start.seconds % 60))\n",
        "\n",
        "# Additional Extensions\n",
        "os.makedirs(patches_dir, exist_ok=True)\n",
        "with capture.capture_output() as cap:\n",
        "  if umiai_ext:\n",
        "    model_url+=\"https://github.com/Klokinator/Umi-AI, \"\n",
        "    !rm -rf /content/stable-diffusion-webui/extensions/sd-dynamic-prompts\n",
        "  else:\n",
        "    if os.path.exists(\"/content/stable-diffusion-webui/extensions/Umi-AI\"):\n",
        "      !rm -rf /content/stable-diffusion-webui/extensions/Umi-AI\n",
        "\n",
        "  # if two_shot_ext:\n",
        "  #   model_url+=\"https://github.com/opparco/stable-diffusion-webui-two-shot, \"\n",
        "  #   !wget https://raw.githubusercontent.com/opparco/stable-diffusion-webui-two-shot/main/cfg_denoised_callback-ea9bd9fc.patch -P {patches_dir}  -c\n",
        "  #   !cd /content/stable-diffusion-webui/ && git apply --ignore-whitespace {patches_dir}/cfg_denoised_callback-ea9bd9fc.patch\n",
        "  # else:\n",
        "  #   if os.path.exists(\"/content/stable-diffusion-webui/extensions/stable-diffusion-webui-two-shot\"):\n",
        "  #     !rm -rf /content/stable-diffusion-webui/extensions/stable-diffusion-webui-two-shot\n",
        "\n",
        "# Revert changes using time-machine (git reset)\n",
        "if commit_hash:\n",
        "  print('‚åö Activating time machine...', end=\"\")\n",
        "  with capture.capture_output() as cap:\n",
        "    %cd /content/stable-diffusion-webui\n",
        "    !git config --global user.email \"you@example.com\"\n",
        "    !git config --global user.name \"Your Name\"\n",
        "    # !git stash\n",
        "    !git reset --hard {commit_hash}\n",
        "    # !git stash apply\n",
        "    # !rm -rf /content/stable-diffusion-webui/embeddings/*\n",
        "    del cap\n",
        "  print('\\r‚åö Time machine activated, you\\'re on commit', commit_hash, flush=True)\n",
        "  # print('‚úåÔ∏è Embeddings have been deleted for time machine support')\n",
        "\n",
        "# Colab patches for quality of life improvements\n",
        "with capture.capture_output() as cap:\n",
        "  # RAM Patch Code by ddPn08: https://github.com/ddPn08/automatic1111-colab/commit/81431d6bd66f0ef96fe638c2743623522e1bc797\n",
        "  !wget https://raw.githubusercontent.com/ddPn08/automatic1111-colab/main/patches/stablediffusion-lowram.patch -P {patches_dir}  -c\n",
        "  if ram_patch_for_sd2: \n",
        "    !cd /content/stable-diffusion-webui/repositories/stable-diffusion-stability-ai && git apply {patches_dir}/stablediffusion-lowram.patch\n",
        "  else:\n",
        "    !cd /content/stable-diffusion-webui/repositories/stable-diffusion-stability-ai && git apply -R {patches_dir}/stablediffusion-lowram.patch\n",
        "\n",
        "  # Colab Optimizations by TheLastBen. Included load in vram + gradio queue with high concurrency_count\n",
        "  if colab_optimizations:\n",
        "    !sed -i \"s@os.path.splitext(checkpoint_.*@os.path.splitext(checkpoint_file); map_location='cuda'@\" /content/stable-diffusion-webui/modules/sd_models.py\n",
        "    !sed -i 's@ui.create_ui().*@ui.create_ui();shared.demo.queue(concurrency_count=999999,status_update_rate=0.1)@' /content/stable-diffusion-webui/webui.py\n",
        "  else:\n",
        "    !sed -i \"s@os.path.splitext(checkpoint_.*@os.path.splitext(checkpoint_file)@\" /content/stable-diffusion-webui/modules/sd_models.py\n",
        "    !sed -i 's@ui.create_ui().*@ui.create_ui()@' /content/stable-diffusion-webui/webui.py\n",
        "\n",
        "  # Merge in vram: self-explainotory \n",
        "  if merge_in_vram:\n",
        "    !sed -i \"s@'cpu'@'cuda'@\" /content/stable-diffusion-webui/modules/extras.py\n",
        "  else:\n",
        "    !sed -i \"s@'cuda'@'cpu'@\" /content/stable-diffusion-webui/modules/extras.py\n",
        "\n",
        "  # Remove custom theme (since it's not for everyone)\n",
        "  if no_custom_theme:\n",
        "    !rm -rf \"/content/stable-diffusion-webui/extensions/sd-web-ui-quickcss/style.css\"\n",
        "  else:\n",
        "    if not os.path.exists('/content/stable-diffusion-webui/extensions/sd-web-ui-quickcss/style.css'):\n",
        "      !cp \"/content/stable-diffusion-webui/extensions/sd-web-ui-quickcss/style_choices/NoCrypt Special.css\" \"/content/stable-diffusion-webui/extensions/sd-web-ui-quickcss/style.css\"\n",
        "\n",
        "# Ngrok stuff goes here\n",
        "if ngrok_token or ngrok_auto_save_load:\n",
        "  if ngrok_auto_save_load:\n",
        "    if not os.path.exists('/content/gdrive'):\n",
        "      drive.mount('/content/gdrive')\n",
        "    if ngrok_token:\n",
        "      if not os.path.exists(\"/content/gdrive/MyDrive/WebUI/ngrokToken.txt\"):\n",
        "        !mkdir -p /content/gdrive/MyDrive/WebUI/\n",
        "        !touch /content/gdrive/MyDrive/WebUI/ngrokToken.txt\n",
        "      f = open(\"/content/gdrive/MyDrive/WebUI/ngrokToken.txt\", \"w+\")\n",
        "      f.write(ngrok_token+\",\"+ngrok_region)\n",
        "      f.close()\n",
        "    elif os.path.exists('/content/gdrive/MyDrive/WebUI/ngrokToken.txt'):\n",
        "      ngrok_token,ngrok_region = getoutput(\"cat /content/gdrive/MyDrive/WebUI/ngrokToken.txt\").split(\",\",2)\n",
        "    else:\n",
        "      print(\"warning: ngrok token not detected\")\n",
        "  commandline_arguments += ' --ngrok ' + ngrok_token + ' --ngrok-region ' + ngrok_region\n",
        "  commandline_arguments = commandline_arguments.replace(\"--share\",\"\")\n",
        "\n",
        "# Configs in drive\n",
        "if configs_in_drive:\n",
        "  config_dir = drive_config_dir+\"config.json\"\n",
        "  if not os.path.exists(drive_config_dir):\n",
        "    !mkdir -p {drive_config_dir}\n",
        "    !cp /content/stable-diffusion-webui/styles.csv /content/stable-diffusion-webui/ui-config.json /content/stable-diffusion-webui/config.json {drive_config_dir}\n",
        "  commandline_arguments += ' --ui-config-file ' + drive_config_dir+\"ui-config.json\"\n",
        "  commandline_arguments += ' --ui-settings-file ' + drive_config_dir+\"config.json\"\n",
        "  commandline_arguments += ' --styles-file ' + drive_config_dir+\"styles.csv\"\n",
        "  \n",
        "# Image outputs to drive (part 2)\n",
        "if output_to_drive:\n",
        "  !sed -i 's@\"outdir_txt2img_samples\": \"outputs/txt2img-images\"@\"outdir_txt2img_samples\": \"/content/gdrive/MyDrive/WebUI/outputs/txt2img-images\"@' {config_dir}\n",
        "  !sed -i 's@\"outdir_img2img_samples\": \"outputs/img2img-images\"@\"outdir_img2img_samples\": \"/content/gdrive/MyDrive/WebUI/outputs/img2img-images\"@' {config_dir}\n",
        "  !sed -i 's@\"outdir_extras_samples\": \"outputs/extras-images\"@\"outdir_extras_samples\": \"/content/gdrive/MyDrive/WebUI/outputs/extras-images\"@' {config_dir}\n",
        "  !sed -i 's@\"outdir_txt2img_grids\": \"outputs/txt2img-grids\"@\"outdir_txt2img_grids\": \"/content/gdrive/MyDrive/WebUI/outputs/txt2img-grids\"@' {config_dir}\n",
        "  !sed -i 's@\"outdir_img2img_grids\": \"outputs/img2img-grids\"@\"outdir_img2img_grids\": \"/content/gdrive/MyDrive/WebUI/outputs/img2img-grids\"@' {config_dir}\n",
        "  !sed -i 's@\"outdir_save\": \"log/images\"@\"outdir_save\": \"/content/gdrive/MyDrive/WebUI/outputs/log/images\"@' {config_dir}\n",
        "else: \n",
        "  if '/gdrive/' in getoutput('cat '+config_dir):\n",
        "    !sed -i 's@\"outdir_txt2img_samples\": \"outputs/txt2img-images\"@\"outdir_txt2img_samples\": \"outputs/txt2img-images\"@' {config_dir}\n",
        "    !sed -i 's@\"outdir_img2img_samples\": \"outputs/img2img-images\"@\"outdir_img2img_samples\": \"outputs/img2img-images\"@' {config_dir}\n",
        "    !sed -i 's@\"outdir_extras_samples\": \"outputs/extras-images\"@\"outdir_extras_samples\": \"outputs/extras-images\"@' {config_dir}\n",
        "    !sed -i 's@\"outdir_txt2img_grids\": \"outputs/txt2img-grids\"@\"outdir_txt2img_grids\": \"outputs/txt2img-grids\"@' {config_dir}\n",
        "    !sed -i 's@\"outdir_img2img_grids\": \"outputs/img2img-grids\"@\"outdir_img2img_grids\": \"outputs/img2img-grids\"@' {config_dir}\n",
        "    !sed -i 's@\"outdir_save\": \"log/images\"@\"outdir_save\": \"log/images\"@' {config_dir}\n",
        "\n",
        "\n",
        "# Install models from model_url, oh boi it's getting bigger\n",
        "extension_repo = []\n",
        "prefixes = [\n",
        "  \"config:\",\n",
        "  \"ui-config:\",\n",
        "  \"styles:\",\n",
        "  \"lora:\",\n",
        "  \"hypernetwork:\",\n",
        "  \"locon:\",\n",
        "  \"model:\",\n",
        "  \"vae:\",\n",
        "  \"control:\",\n",
        "  \"clone:\",\n",
        "  \"gfpgan:\",\n",
        "  \"ersgan:\",\n",
        "  \"swinr:\",\n",
        "  \"ldsr:\",\n",
        "  \"repo:\",\n",
        "  \"embeddings:\"\n",
        "]\n",
        "token = optional_huggingface_token if optional_huggingface_token else \"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\"\n",
        "user_header = f\"\\\"Authorization: Bearer {token}\\\"\"\n",
        "print('üì¶ Downloading models and stuff...', end='')\n",
        "def handle_manual(url):\n",
        "  if url.startswith(\"config:\"):\n",
        "    manual_download(url, \"/content/stable-diffusion-webui/config.json\")\n",
        "  elif url.startswith(\"ui-config:\"):\n",
        "    manual_download(url, \"/content/stable-diffusion-webui/ui-config.json\")\n",
        "  elif url.startswith(\"styles:\"):\n",
        "    manual_download(url, \"/content/stable-diffusion-webui/styles.csv\")\n",
        "  elif url.startswith(\"lora:\") or url.startswith(\"locon:\"):\n",
        "    manual_download(url, loras_dir)\n",
        "  elif url.startswith(\"hypernetwork:\"):\n",
        "    manual_download(url, hypernetworks_dir)\n",
        "  elif url.startswith(\"model:\"):\n",
        "    manual_download(url, models_dir)\n",
        "  elif url.startswith(\"vae:\"):\n",
        "    manual_download(url, vaes_dir)\n",
        "  elif url.startswith(\"control:\"):\n",
        "    manual_download(url, control_dir)\n",
        "  elif url.startswith(\"gfpgan:\"):\n",
        "    manual_download(url, \"/content/stable-diffusion-webui/models/GFPGAN\")\n",
        "  elif url.startswith(\"ersgan:\"):\n",
        "    manual_download(url, \"/content/stable-diffusion-webui/models/ERSGAN\")\n",
        "  elif url.startswith(\"swinr:\"):\n",
        "    manual_download(url, \"/content/stable-diffusion-webui/models/SwinR\")\n",
        "  elif url.startswith(\"ldsr:\"):\n",
        "    manual_download(url, \"/content/stable-diffusion-webui/models/LDSR\")\n",
        "  elif url.startswith(\"embeddings:\"):\n",
        "    manual_download(url, embeddings_dir)\n",
        "  elif url.startswith(\"extension:\"):\n",
        "    extension_repo.append(url)\n",
        "  elif url.startswith(\"clone:\") or url.startswith(\"repo:\"): \n",
        "    !cd /content/.downloaded && git clone $url \n",
        "\n",
        "def manual_download(url, dst):\n",
        "  url = url[url.find(':')+1:]\n",
        "  if \".json\" in url or \".csv\" in url:\n",
        "    !wget \"{url}\" -O {dst} -c\n",
        "  elif '.yaml' in url or '.yml' in url or 'discord' in url:\n",
        "    !wget \"{url}\" -P {dst} -c\n",
        "  elif 'drive.google' in url:\n",
        "    if 'folders' in url:\n",
        "      !gdown --folder \"{url}\" -O {dst} --fuzzy -c\n",
        "    else:\n",
        "      !gdown \"{url}\" -O {dst} --fuzzy -c\n",
        "  elif 'huggingface' in url:\n",
        "    if '/blob/' in url:\n",
        "      url = url.replace('/blob/', '/resolve/')\n",
        "    parsed_link = '\\n{}\\n\\tout={}'.format(url,unquote(url.split('/')[-1]))\n",
        "    !echo -e \"{parsed_link}\" | aria2c --header={user_header} --console-log-level=error --summary-interval=10 -i- -j5 -x16 -s16 -k1M -c -d \"{dst}\" \n",
        "  elif 'http' in url or 'magnet' in url:\n",
        "    parsed_link = '\"{}\"'.format(url)\n",
        "    !aria2c --optimize-concurrent-downloads --console-log-level=error --summary-interval=10 -j5 -x16 -s16 -k1M -c -d {dst} -Z {parsed_link}\n",
        "\n",
        "def download(url):\n",
        "  try:\n",
        "    have_drive_link\n",
        "  except:\n",
        "    if \"drive.google.com\" in url:\n",
        "      # I'm sorry drive ID enjoyer, this will make ID useless :(\n",
        "      !pip install -U gdown\n",
        "      have_drive_link = True\n",
        "  links_and_paths = url.split(',')\n",
        "  !mkdir -p {destination_dir} {models_dir} {vaes_dir} {hypernetworks_dir} {embeddings_dir} {loras_dir}\n",
        "  http_links = []\n",
        "  huggingface_links = []\n",
        "  for link_or_path in links_and_paths:\n",
        "    link_or_path = link_or_path.strip()\n",
        "    if not link_or_path:\n",
        "      continue\n",
        "\n",
        "    if any(link_or_path.startswith(prefix.lower()) for prefix in prefixes):\n",
        "      handle_manual(link_or_path)\n",
        "      continue\n",
        "\n",
        "    if 'github.com' in link_or_path and ( '.git' in link_or_path or not '.' in link_or_path.split('/')[-1] ):\n",
        "      extension_repo.append(link_or_path)\n",
        "      continue\n",
        "      \n",
        "    if '.yaml' in link_or_path or '.yml' in link_or_path or 'discord' in link_or_path:\n",
        "      !wget {link_or_path} -P {destination_dir} -c\n",
        "    elif 'drive.google' in link_or_path:\n",
        "      if 'folders' in link_or_path:\n",
        "        !gdown --folder {link_or_path} -O {destination_dir} --fuzzy -c\n",
        "      else:\n",
        "        !gdown {link_or_path} -O {destination_dir} --fuzzy -c\n",
        "    elif 'huggingface' in link_or_path:\n",
        "      if '/blob/' in link_or_path:\n",
        "        link_or_path = link_or_path.replace('/blob/', '/resolve/')\n",
        "      huggingface_links.append(link_or_path)\n",
        "    elif 'http' in link_or_path or 'magnet' in link_or_path:\n",
        "      http_links.append(link_or_path)\n",
        "    elif '/' in link_or_path:\n",
        "      if not os.path.exists('/content/gdrive/MyDrive'):\n",
        "        print('Looks like there\\'s a path in your url. You need to mount your drive first.')\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/gdrive')\n",
        "      !rsync -avr --progress /content/gdrive/MyDrive/{link_or_path} {destination_dir}\n",
        "    else:\n",
        "      !gdown {link_or_path} -O {destination_dir} --fuzzy -c\n",
        "  if http_links:\n",
        "    links_string = ' '.join(['\"{}\"'.format(x) for x in http_links])\n",
        "    !aria2c --optimize-concurrent-downloads --console-log-level=error --summary-interval=10 -j5 -x16 -s16 -k1M -c -d {destination_dir}  -Z {links_string}\n",
        "    del links_string\n",
        "  if huggingface_links:\n",
        "    # links_string = ' '.join(['\"{}\"'.format(x) for x in huggingface_links])\n",
        "    # !aria2c --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {destination_dir} -Z {links_string}\n",
        "    # for link in huggingface_links:\n",
        "    #   !aria2c --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {destination_dir} -o {link.split('/')[-1]} {link}\n",
        "    links_string = '\\n'.join(['{}\\n\\tout={}'.format(x,unquote(x.split('/')[-1])) for x in huggingface_links])  \n",
        "    !echo -e \"{links_string}\" | aria2c --header={user_header} --optimize-concurrent-downloads --console-log-level=error --summary-interval=10 -i- -j5 -x16 -s16 -k1M -c -d {destination_dir} \n",
        "\n",
        "if verbose_download:\n",
        "  download(model_url)\n",
        "else:\n",
        "  with capture.capture_output() as cap:\n",
        "    download(model_url)\n",
        "    del cap\n",
        "\n",
        "print('\\rüèÅ Download finished.', flush=True)\n",
        "\n",
        "if len(extension_repo) > 0:\n",
        "  print('‚ú® Installing custom extensions...', end='')\n",
        "  with capture.capture_output() as cap:\n",
        "    for repo in extension_repo:\n",
        "      repo_name = repo.split('/')[-1]\n",
        "      !cd {extensions_dir} \\\n",
        "        && git clone \"{repo}\" \\\n",
        "        && cd {repo_name} \\\n",
        "        && git fetch\n",
        "  print('\\rüèÅ Installed',len(extension_repo),'custom extensions.', flush=True)\n",
        "\n",
        "print('\\n')\n",
        "# Link all files by filtering accoridng to their type\n",
        "with capture.capture_output() as cap:\n",
        "  # files = os.listdir(destination_dir)\n",
        "  files = [os.path.join(dp,f) for dp, dn, fn in os.walk(destination_dir) for f in fn] # Thanks Aojiru!\n",
        "  for file in files:\n",
        "    name, file_extension = os.path.splitext(file)\n",
        "    if '.aria2' in file:\n",
        "      continue\n",
        "    file_path = os.path.join(destination_dir, file)\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    if \"control_\" in name or \"t2iadapter_\" in name or file_extension == \".pth\":\n",
        "      !ln \"{file_path}\" {control_dir}\n",
        "    elif file_extension in ['.yaml', '.yml'] or file_size > 1_500_000_000: \n",
        "      !ln \"{file_path}\" {models_dir}\n",
        "    elif \"kl-f8\" in name or \"vae_\" in file or \"vae.\" in file or \"vae-\" in file or file_size > 380_000_000: \n",
        "      !ln \"{file_path}\" {vaes_dir}\n",
        "    elif getoutput('if rg -q -o \"lora_unet\" \"'+file_path+'\"; then echo 1; else echo 0; fi') == \"1\":\n",
        "      !ln \"{file_path}\" {loras_dir}\n",
        "    elif (file_extension == '.pt' or file_extension == '.safetensors') and file_size < 10_000_000:\n",
        "      !ln \"{file_path}\" {embeddings_dir}\n",
        "    else:\n",
        "      !ln \"{file_path}\" {hypernetworks_dir}\n",
        "  del cap\n",
        "\n",
        "# Automatically loads vae for first run, if it exists.\n",
        "if auto_vae:\n",
        "  if '.vae.pt' in os.listdir(vaes_dir) or '/vae' in model_url:\n",
        "    commandline_arguments+=' --vae-path $(readlink -f $(find '+vaes_dir+' \\( -name \"*.vae.pt\" -or -name \"*.ckpt\" \\) -print -quit))'\n",
        "\n",
        "# Configure Alternatives Tunnels (Colab native, localtunnel, cloudflared, bore with auth)\n",
        "if alternative_tunnels:\n",
        "  commandline_arguments = commandline_arguments.replace(\"--share\",\"\")\n",
        "  print(\"‚åö \\033[95m\\033[1mGenerating alternative tunnels...\", end='')\n",
        "  with capture.capture_output() as cap:\n",
        "    %cd /content\n",
        "    if not os.path.exists('/tools/node/bin/lt'):\n",
        "      !npm install -g localtunnel\n",
        "    if not os.path.exists('/usr/bin/cloudflared'):\n",
        "      !curl -Lo /usr/bin/cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 && chmod +x /usr/bin/cloudflared\n",
        "    del cap\n",
        "  !true > /content/nohup.out\n",
        "  !nohup lt --port 7860 > /content/nohup.out 2>&1 &\n",
        "  !nohup cloudflared tunnel --url localhost:7860 > /content/nohup.out 2>&1 &\n",
        "  if with_bore:\n",
        "    if not os.path.exists('/usr/bin/bore'):\n",
        "      !curl -Ls https://github.com/ekzhang/bore/releases/download/v0.4.0/bore-v0.4.0-x86_64-unknown-linux-musl.tar.gz | tar zx -C /usr/bin\n",
        "    !nohup bore local 7860 --to bore.pub > /content/nohup.out 2>&1 &\n",
        "    if not \"--gradio-auth\" in commandline_arguments:\n",
        "      import random\n",
        "      import string\n",
        "      gradio_password = ''.join(random.choice(string.ascii_lowercase) for i in range(5))\n",
        "      commandline_arguments+=\" --gradio-auth {}:{}\".format(\"ncpt\", gradio_password)\n",
        "    else:\n",
        "      gradio_password = False\n",
        "  !sleep 4\n",
        "  print(\"\\rüí° \\033[95m\\033[1mUse one of these alternative tunnels after the loading is finished: \", flush=True)\n",
        "  from google.colab.output import serve_kernel_port_as_window\n",
        "  serve_kernel_port_as_window(7860, anchor_text=\"https://th15f4k3l1nkofcn0tr34ll0l-7860-colab.googleusercontent.com/\")\n",
        "  !cat /content/nohup.out | rg -a -o \"https[^ ]*.*\\.trycloudflare\\.com|https[^ ]*.*\\.loca\\.lt|bore.pub:[^ ]*\" | sed 's@bore.pub@http://bore.pub@'\n",
        "  print(\"\\n\")\n",
        "  if with_bore:\n",
        "    if gradio_password:\n",
        "      print(\"\\rüîê \\033[0m\\033[1mLooks like you're using bore without --gradio-auth huh... \")\n",
        "      print(\"For security, I've enforced to use gradio auth, so use this account to login:\")\n",
        "      print(\"üëâ‚ö†Ô∏è Username: ncpt\")\n",
        "      print(\"üëâ‚ö†Ô∏è Password:\", gradio_password,\"\\n\\n\")\n",
        "\n",
        "# If no xformers installed, remove --xformers from arg to avoid using old builtin xformers \n",
        "# if not os.path.exists(\"/usr/local/lib/python3.9/dist-packages/xformers\"):\n",
        "#   commandline_arguments = commandline_arguments.replace(\"--xformers\",\"\")\n",
        "\n",
        "# Krita extension support (adding --api automatically) + toggleable.\n",
        "with capture.capture_output() as cap: \n",
        "  if krita_paint_ext:\n",
        "    # Add api if no api (if lazy y'know)\n",
        "    if not \"--api\" in commandline_arguments:\n",
        "      commandline_arguments+=\" --api\"\n",
        "    %cd /content/stable-diffusion-webui\n",
        "    if os.path.exists('/content/stable-diffusion-webui/extensions/auto-sd-paint-ext'):\n",
        "      !cd ./extensions/auto-sd-paint-ext && git fetch && git merge # Deflecting FETCH_HEAD not found bug\n",
        "    else:\n",
        "      !git clone https://github.com/Interpause/auto-sd-paint-ext extensions/auto-sd-paint-ext\n",
        "  else:\n",
        "    if os.path.exists('/content/stable-diffusion-webui/extensions/auto-sd-paint-ext'):\n",
        "      !rm -rf /content/stable-diffusion-webui/extensions/auto-sd-paint-ext\n",
        "  # Remove junks\n",
        "  !find /content/stable-diffusion-webui/ -name \".ipynb_checkpoints\" -type d -exec rm -r {} \\;\n",
        "\n",
        "\n",
        "# Print all files in every important directory\n",
        "print(\"\\033[96mCan't see your files in here? Activate verbose_download to debug!\\n\")\n",
        "print(\"\\033[92m\\033[1m‚ï≠-üì¶ Models + Configs\\033[96m\")\n",
        "!find {models_dir}/ -mindepth 1 ! -name '*.txt' -printf '%f\\n' \n",
        "print(\"\\n\\033[92m\\033[1m‚ï≠-üì¶ VAEs\\033[96m\")\n",
        "!find {vaes_dir}/ -mindepth 1 ! -name '*.txt' -printf '%f\\n'\n",
        "print(\"\\n\\033[92m\\033[1m‚ï≠-üì¶ Custom Embeddings (inbuilt is hidden)\\033[96m\")\n",
        "!find {embeddings_dir}/ -mindepth 1 -maxdepth 1 -name '*.pt' -or -name '*.safetensors' -printf '%f\\n'\n",
        "print(\"\\n\\033[92m\\033[1m‚ï≠-üì¶ LoRAs\\033[96m\")\n",
        "!find {loras_dir}/ -mindepth 1 ! -name '*.keep' -printf '%f\\n'\n",
        "print(\"\\n\\033[92m\\033[1m‚ï≠-üì¶ Hypernetworks\\033[96m\")\n",
        "!find {hypernetworks_dir}/ -mindepth 1 ! -name '*.txt' -printf '%f\\n'\n",
        "print(\"\\n\\033[92m\\033[1m‚ï≠-üì¶ Extensions\\033[96m\")\n",
        "!find {extensions_dir}/ -mindepth 1 -maxdepth 1 ! -name '*.txt' -printf '%f\\n'\n",
        "print(\"\\n\\n\\033[0m\")\n",
        "\n",
        "# Start the webui\n",
        "%cd /content/stable-diffusion-webui\n",
        "\n",
        "if null_model:\n",
        "  print(\"\\033[91m ‚ö†Ô∏è Null model will be loaded, if you don't understand please uncheck the model \\033[0m\")\n",
        "  !wget https://huggingface.co/ckpt/null/resolve/main/nullModelzeros.ckpt -P {models_dir} -c\n",
        "  commandline_arguments+=\" --ckpt \"+models_dir+\"/nullModelzeros.ckpt \"\n",
        "\n",
        "if fast_start:\n",
        "  # commandline_arguments += \" --skip-install\"\n",
        "  print(\"\\033[91m ‚ö†Ô∏è Fast start is active, please disable it if you have any problem! \\033[0m\")\n",
        "  !python webui.py $commandline_arguments\n",
        "else:\n",
        "  !COMMANDLINE_ARGS=\"{commandline_arguments}\" REQS_FILE=\"requirements_versions.txt\" python launch.py \n",
        "time_since_start = timedelta(seconds=time.time()-start_colab)\n",
        "print(\"\\n\\n\\033[96m‚åö You've been running this colab for\",\"%02d:%02d:%02d\" % (time_since_start.seconds / 3600, (time_since_start.seconds / 60) % 60, time_since_start.seconds % 60))\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Change to PyTorch 2.0 `(For testing)`\n",
        "#@markdown Feeling brave? Wanna try something cutting edge? This is for you.\n",
        "!pip uninstall -y xformers triton\n",
        "!pip3 install --force-reinstall --pre torch torchtext torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu116\n",
        "!pip install https://github.com/NoCrypt/xformers_colab/releases/download/3969054437/xformers-0.0.16+bc08bbc.d20230120-cp38-cp38-linux_x86_64.whl\n",
        "!python -m xformers.info"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uqqdiSr3MFA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **Useful Utilities (Optional)**\n",
        "\n",
        "*Stuff that might be useful*"
      ],
      "metadata": {
        "id": "iLsO_riJrAEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Colab Mobile Keep Alive \n",
        "#@markdown Run this cell to keep the tab alive in mobile (before running the start cell)\n",
        "\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive before running the start cell (Uses only 13MB of data)</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ],
      "metadata": {
        "id": "ZIL7itnNaw5V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Mass download outputs\n",
        "#@markdown Zip all image outputs and downloads them\n",
        "from google.colab import files\n",
        "import os\n",
        "%cd /content\n",
        "if output_to_drive:\n",
        "  print(\"Your images already in google drive btw, this cell is basically useless but what ever\")\n",
        "  \n",
        "if os.path.exists(\"/content/outputs.zip\"):\n",
        "  !rm -f /content/outputs.zip\n",
        "!cd /content/stable-diffusion-webui/outputs && zip -r /content/outputs.zip .\n",
        "files.download('/content/outputs.zip')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "vcca4v99sBd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XTerm Terminal\n",
        "*Spins up a terminal*"
      ],
      "metadata": {
        "id": "QAvV1XNM3TxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install XTerm\n",
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "xaIJuwU1f3k7",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run XTerm (Terminal)\n",
        "%xterm"
      ],
      "metadata": {
        "id": "Umxr9M53f45V",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload stuff\n",
        "*Quickly upload stuff like merged models to huggingface*\n",
        "\n",
        "Run cell `1,2,3`\n",
        "\n",
        "You only need to choose one between `3.1,3.2,and 3.3`\n",
        "\n",
        "<small>Forked from LYNN\n"
      ],
      "metadata": {
        "id": "EzbWAsEc2bAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 1. Login to Huggingface hub\n",
        "try:\n",
        "  hub_ok # My packaged dep didn't contains this (but stil have the folder)\n",
        "except:\n",
        "  print(\"Setting up huggingface_hub...\")\n",
        "  !pip install --force-reinstall -qqqq huggingface_hub\n",
        "  hub_ok = True\n",
        "from IPython.display import clear_output\n",
        "from huggingface_hub import login\n",
        "clear_output()\n",
        "\n",
        "#@markdown 1. Of course, you need a Huggingface account first.\n",
        "#@markdown 2. To create a huggingface token, go to [this link](https://huggingface.co/settings/tokens), then `create new token` or copy available token with the `Write` role.\n",
        "\n",
        "write_token = \"\" #@param {type:\"string\"}\n",
        "login(write_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HLPOBeoG2hIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2. Setup Repo\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "\n",
        "api = HfApi()\n",
        "user = api.whoami(write_token)\n",
        "\n",
        "#@markdown #### If your model repo didn't exist, it will automatically create your repo.\n",
        "repo_name = \"my_merged_models\" #@param{type:\"string\"}\n",
        "make_this_repo_private_if_not_exist = False #@param{type:\"boolean\"}\n",
        "clone_with_git = True #@param{type:\"boolean\"}\n",
        "\n",
        "model_repo = user['name']+\"/\"+repo_name.strip()\n",
        "\n",
        "validate_repo_id(model_repo)\n",
        "\n",
        "if repo_name != \"\":\n",
        "  try:\n",
        "      api.create_repo(repo_id=model_repo, \n",
        "                      private=make_this_repo_private_if_not_exist)\n",
        "      print(\"Model Repo didn't exists, creating repo\")\n",
        "      print(\"Model Repo: \",model_repo,\"created!\\n\")\n",
        "\n",
        "  except HfHubHTTPError as e:\n",
        "      print(f\"Model Repo: {model_repo} exists, skipping create repo\\n\")\n",
        "\n",
        "if clone_with_git:\n",
        "  !git lfs install --skip-smudge\n",
        "  !export GIT_LFS_SKIP_SMUDGE=1\n",
        "  !git clone https://huggingface.co/{model_repo} /content/{repo_name}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "3ecSNSOY3rKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xSLziGY-sa89"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import clear_output\n",
        "from huggingface_hub import HfApi, login\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "from IPython.utils import capture\n",
        "\n",
        "\n",
        "paths_map = {\n",
        "    \"Models\" : \"/content/stable-diffusion-webui/models/Stable-diffusion\",\n",
        "    \"VAEs\" : \"/content/stable-diffusion-webui/models/VAE\",\n",
        "    \"LORAs\" : \"/content/stable-diffusion-webui/models/Lora\",\n",
        "    \"Embeddings\" : \"/content/stable-diffusion-webui/embeddings\",\n",
        "    \"Hypernetworks\" : \"/content/stable-diffusion-webui/models/hypernetworks\",\n",
        "}\n",
        "\n",
        "#@title  3.1 Upload via huggingface_hub (Fun way) \n",
        "#@markdown ## **How to use this?**<br>\n",
        "#@markdown 1. Run this cell after you ran the login cell\n",
        "#@markdown 2. Select model you want to upload (use `ctrl/shift` for multiple selection)\n",
        "#@markdown 3. Click on upload button\n",
        "folder = \"Models\" #@param [\"Models\", \"VAEs\", \"LORAs\", \"Embeddings\", \"Hypernetworks\"]\n",
        "commit_message = \"Upload with üöÄü§ó NoCrypt's nocrypt_colab_remastered\"  #@param{type:\"string\"}\n",
        "\n",
        "models_path = paths_map[folder]\n",
        "upload_path = '/content/upload_models'\n",
        "\n",
        "api = HfApi()\n",
        "username_repo = user['name']+\"/\"+repo_name.strip()\n",
        "validate_repo_id(username_repo)\n",
        "\n",
        "def get_file_list(path):\n",
        "  res = []\n",
        "  for (dir_path, dir_names, file_names) in os.walk(path):\n",
        "      res.extend(file_names)\n",
        "  return res\n",
        "  \n",
        "selected = widgets.SelectMultiple(\n",
        "    options=get_file_list(models_path),\n",
        "    rows=10,\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "button = widgets.Button(\n",
        "    description='Upload',\n",
        "    disabled=False,\n",
        "    button_style='success',\n",
        "    tooltip='Upload to huggingface',\n",
        ")\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=paths_map.keys(),\n",
        "    value=folder,\n",
        "    description='Folder',\n",
        ")\n",
        "\n",
        "out = widgets.Output()\n",
        "\n",
        "def on_folder_change(change):\n",
        "  if change['type'] == 'change' and change['name'] == 'value':\n",
        "      models_path = paths_map[change[\"new\"]]\n",
        "      selected.options = get_file_list(models_path)\n",
        "\n",
        "def upload_it(b):\n",
        "  with out:\n",
        "    if selected.value is not None:\n",
        "      clear_output()\n",
        "      !mkdir -p {upload_path}\n",
        "      \n",
        "      \n",
        "      #hard link each file\n",
        "      for selected_model in selected.value:\n",
        "        if not os.path.exists(os.path.join(upload_path,selected_model)):\n",
        "          os.link(os.path.join(paths_map[dropdown.value],selected_model),os.path.join(upload_path,selected_model)) #hardlinking to save colab's space\n",
        "      \n",
        "      #delete .ipynb_checkpoint\n",
        "      if os.path.exists(os.path.join(upload_path,\".ipynb_checkpoints\")):\n",
        "        !rm {upload_path}/.ipynb_checkpoints\n",
        "      print(\"Selected:\", \", \".join(selected.value))\n",
        "      print(\"Uploading to https://huggingface.co/\"+username_repo)\n",
        "      print(\"Please wait... Might look stuck, but it's not üëç\")\n",
        "\n",
        "      # Comment this for file based upload\n",
        "      api.upload_folder(\n",
        "          folder_path=upload_path,\n",
        "          repo_id=username_repo,\n",
        "          commit_message=commit_message\n",
        "      )\n",
        "\n",
        "      # Uncomment for file based upload\n",
        "      # for filename in os.listdir(upload_path):\n",
        "      #   f = os.path.join(upload_path, filename)\n",
        "      #   # checking if it is a file\n",
        "      #   if os.path.isfile(f):\n",
        "      #     api.upload_file(\n",
        "      #       path_or_fileobj=f,\n",
        "      #       path_in_repo=filename,\n",
        "      #       repo_id=username_repo,\n",
        "      #       commit_message=commit_message\n",
        "      #     )\n",
        "      \n",
        "      print(\"Done!\")\n",
        "      #delete hardlink\n",
        "      !rm -rf {upload_path}/*\n",
        "    else:\n",
        "      print(\"Nothing is selected\")\n",
        "      \n",
        "dropdown.observe(on_folder_change)\n",
        "button.on_click(upload_it)\n",
        "print(\"Upload target: https://huggingface.co/\"+username_repo)\n",
        "print(\"üëá Select models you want to upload (use ctrl/shift for multiple selection) \")\n",
        "display(dropdown,selected,button,out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.2 Upload via huggingface_hub (Manual way)\n",
        "\n",
        "#@markdown All paths for easier access:<br>\n",
        "#@markdown /content/stable-diffusion-webui/models/Stable-diffusion<br>\n",
        "#@markdown /content/stable-diffusion-webui/models/VAE<br>\n",
        "#@markdown /content/stable-diffusion-webui/models/Lora<br>\n",
        "#@markdown /content/stable-diffusion-webui/embeddings<br>\n",
        "#@markdown /content/stable-diffusion-webui/models/hypernetwork<br>\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "\n",
        "api = HfApi()\n",
        "file_path = \"/content/models/Anything-V3-vae-swapped.safetensors\" #@param {type :\"string\"}\n",
        "commit_message = \"Upload with üöÄü§ó NoCrypt's nocrypt_colab_remastered\" #@param {type :\"string\"}\n",
        "\n",
        "if file_path != \"\":\n",
        "  path_obj = Path(file_path)\n",
        "  trained_model = path_obj.parts[-1]\n",
        "\n",
        "  print(f\"Uploading {trained_model} to https://huggingface.co/\"+model_repo)\n",
        "  print(f\"Please wait...\")\n",
        "\n",
        "  api.upload_file(\n",
        "      path_or_fileobj=file_path,\n",
        "      path_in_repo=trained_model,\n",
        "      repo_id=model_repo,\n",
        "      commit_message=commit_message,\n",
        "  )\n",
        "  \n",
        "  print(f\"Upload success, located at https://huggingface.co/\"+model_repo+\"/blob/main/\"+trained_model+\"\\n\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7Juh7N1c3ooM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 3.3 Upload via GIT (more stable)\n",
        "%cd /content/\n",
        "file_path = \"/content/models/Anything-V3-vae-swapped.safetensors\" #@param {type :\"string\"}\n",
        "!ln {file_path} /content/{repo_name}/\n",
        "#@markdown Set **git commit identity**\n",
        "email = \"your-email\" #@param {'type': 'string'}\n",
        "name = \"your-username\" #@param {'type': 'string'}\n",
        "#@markdown Set **commit message**\n",
        "commit_m = \"Upload with üöÄü§ó NoCrypt's nocrypt_colab_remastered\" #@param {'type': 'string'}\n",
        "\n",
        "%cd /content/{repo_name}\n",
        "\n",
        "!git lfs install --skip-smudge\n",
        "!export GIT_LFS_SKIP_SMUDGE=1\n",
        "!git pull -X theirs\n",
        "\n",
        "!git lfs install\n",
        "!huggingface-cli lfs-enable-largefiles .\n",
        "!git config --global user.email \"{email}\"\n",
        "!git config --global user.name \"{name}\"\n",
        "!git add .\n",
        "!git commit -m \"{commit_m}\"\n",
        "!git push"
      ],
      "metadata": {
        "cellView": "form",
        "id": "C4sGvmEf1gIv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}